# Вариант 10
Датасет YearPredictionMSD

# Загрузка dataset
```
def load_yearpredictionmsd_data(file_path):
    try:
        data = pd.read_csv(file_path, header=None, nrows=100000)

        X_df = data.iloc[:, 1:]
        y_df = data.iloc[:, 0]

        X_df = X_df.astype(float)
        y_df = y_df.astype(float)

        return X_df, y_df

    except FileNotFoundError:
        print(f"Файл не найден")
        raise
    except Exception as e:
        print(f"Ошибка при загрузке данных")
        raise
```
Данные загружаются из файла, расположенного в репозитории проекта. Загружаются только первые 10000 строк, поскольку при использовании всех данных время работы программы может превышать 1 час на вывод одного графика.

В моделе рассматриваются полиномиальные признаки 1 и 2 степени, поскольку размеры датасета очень большие и имеют порядка 90 исходных полиномиальных признаков, что сильно увеличивает время работы. 3 и последующии степени начинают вызывать переобучение модели.

Степень 1 - линейная регрессия без полиномиальных признаков
Степень 2 - полиномиальная регрессия с квадратичными членами и взаимодействиями

# Разделение исходной выборки на обучающую и тестовую в соотношении (20/80)

Данные разделяются в пропорции 80/20. Разделение происходит путем перемешивания данных и разделения их по пропорции.

Загрузка данных из файла
```

file_path = "YearPredictionMSD.txt"

    X_df, y_df = load_yearpredictionmsd_data(file_path)

    print(f"Загружено: {X_df.shape[0]} samples")
    print(f"Количество признаков: {X_df.shape[1]}")
    print(f"Диапазон лет: {y_df.min()} - {y_df.max()}")

    return X_df, y_df

X, y = load_and_prepare_data()

```
Определяется количество образцов
```

n_samples = X.shape[0]

```
Создание и присвоение индексов для образцов
```

indices = np.arange(n_samples)

```
Перемешивание образцов
```

np.random.shuffle(indices)

```
Переиндексация образцов с сохранением связи между столбцами
```

X_shuffled = X.iloc[indices]
y_shuffled = y.iloc[indices]

```
Выделение 80% для обучающей выборки
```

train_size = int(n_samples * 0.8)

```
Распределение данных на обучающую и тестовую выборки
```

X_train = X_shuffled[:train_size]
X_test = X_shuffled[train_size:]

y_train = y_shuffled[:train_size]
y_test = y_shuffled[train_size:]

```

# Линейная регрессия
Линейная регрессия - статистический метод моделирования зависимости между независимыми переменными (признаками) и зависимой переменной (целевой величиной).

С использованием библиотеки scikit-learn обучаем модель линейной регрессии по обучающей выборке

Создаём объект model класса LinearRegression и применяем метод fit() для обучения модели.

Затем тестируем нашу модель на обучающей и тестовой выборке.

R² (коэффициент детерминации) – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить

R² показывает:

1.0 - идеальное предсказание

0.0 - модель не лучше среднего значения

< 0 - модель хуже простого среднего

Обучение модели линейной регрессии
```

from sklearn.linear_model import LinearRegression

regressor = LinearRegression().fit(X_train, y_train)

y_train_pred = regressor.predict(X_train)
y_test_pred = regressor.predict(X_test)

```

# Проверка точности модели по тестовой выборке
Проверка точности - это процесс оценки способности модели делать правильные предсказания на новых, ранее не виденных данных.

Для оценки качества модели будем использовать коэффициент детерминации.

Коэффициент детерминации – это нормированная метрика, показывающая, какую долю изменчивости целевой переменной модель смогла объяснить. R² всегда находится в диапазоне от минус бесконечности до 1, где 1 означает идеальное предсказание. Благодаря своей нормированности, R² позволяет сравнивать модели независимо от масштаба данных, что делает его универсальным инструментом оценки.

Оценка модели
```

print(f"Коэфф. детерминации обучающей выборки: {r2_score(y_train, y_train_pred):.2f}")
print(f"Коэфф. детерминации тестовой выборки: {r2_score(y_test, y_test_pred):.2f}")

```

# Модель с использованием полиномиальной функции
Полиномиальная регрессия — это метод машинного обучения, используемый для моделирования нелинейных зависимостей между переменными путем аппроксимации данных полиномом степени (k). В отличие от линейной регрессии, которая предполагает прямую связь, этот метод позволяет учитывать более сложные криволинейные тренды в данных, что делает его полезным в различных областях, таких как экономика и инженерия.

Создаём pipeline - объект, который автоматизирует процесс трансформации признаков в полиномиальные. Он будет применять шаги poly_features и linear_regression по порядку.

Обучение и тестирование модели с полиномиальными признаками 1 и 2 степени
```

degrees = range(1, 3)
r2_train_list = []
r2_test_list = []

for degree in degrees:
    print(f"Обучение с полиномиальными признаками степени {degree}")
    pipeline = Pipeline([
        ("poly_features", PolynomialFeatures(degree=degree, include_bias=False)),
        ("linear_regression", LinearRegression())
    ])

    pipeline.fit(X_train, y_train)

    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    r2_train_list.append(r2_score(y_train, y_train_pred))
    r2_test_list.append(r2_score(y_test, y_test_pred))

plt.figure(figsize=(8, 5))
plt.plot(degrees, r2_train_list, marker='o', label="Train R2")
plt.plot(degrees, r2_test_list, marker='o', label="Test R2")
plt.xlabel("Степень полиномиальных признаков")
plt.ylabel("R^2")
plt.legend()
plt.grid(True)
plt.show()

```

На графике наблюдается переобучение с полиномиальными признаками 2 степени

# Построить модель с использованием регуляризации
Ridge-регрессия (или гребневая регрессия) — это регуляризованная версия линейной регрессии, которая используется для борьбы с мультиколлинеарностью (линейной зависимостью между предикторами) и переобучением. Она добавляет к функции потерь штрафное слагаемое в виде квадрата коэффициентов, чтобы сделать веса модели меньше, но не обнулить их, в отличие от Lasso-регрессии.

Алгоритм обучения пытается найти баланс между подгонкой под данные и поддержанием коэффициентов на низком уровне.

Для борьбы с переобучением и коррелированными признаками применим регуляризацию Ridge (L2). Она добавляет штраф к сумме квадратов коэффициентов модели, что стабилизирует обучение и уменьшает влияние сильно коррелированных признаков.

Полиномиальные признаки сильно коррелированы между собой.
Ridge лучше справляется с мультиколлинеарностью и сохраняет все признаки.

Обучение и тестирование модели гребневой регрессии
```

degree = 2
alphas = np.logspace(-4, 3, 10)

r2_train_list = []
r2_test_list = []

for alpha in alphas:
    pipeline = Pipeline([
        ("poly", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
        ("scaler", StandardScaler()),
        ("ridge", Ridge(alpha=alpha, max_iter=10000))
    ])

    pipeline.fit(X_train, y_train)

    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)

    r2_train_list.append(r2_score(y_train, y_train_pred))
    r2_test_list.append(r2_score(y_test, y_test_pred))

plt.figure(figsize=(8, 5))
plt.semilogx(alphas, r2_train_list, marker='o', label="Train R^2")
plt.semilogx(alphas, r2_test_list, marker='o', label="Test R^2")
plt.xlabel("Коэфф. регуляризации")
plt.ylabel("R^2")
plt.title(f"Гребневая регрессия (Степень = {degree})")
plt.ylim(0, 1)
plt.grid(True)
plt.legend()
plt.show()

best_index = np.argmax(r2_test_list)
best_alpha = alphas[best_index]

print(f"Наилучший коэфф. регуляризации: {best_alpha:.4f}")

```
Для подбора оптимального значения α выбираем диапазон от 10⁻⁴ до 10³ в логарифмической шкале.На графике можно увидеть, при каком αlpha модель лучше всего балансирует между переобучением и недообучением. Оптимальное значение α соответствует максимальному R² на тестовой выборке.
